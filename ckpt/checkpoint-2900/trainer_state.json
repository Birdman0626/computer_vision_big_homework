{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 2900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.13793103448275862,
      "grad_norm": 0.008071549236774445,
      "learning_rate": 0.496551724137931,
      "loss": 2.1377,
      "step": 20
    },
    {
      "epoch": 0.27586206896551724,
      "grad_norm": 0.0026637085247784853,
      "learning_rate": 0.49310344827586206,
      "loss": 2.0972,
      "step": 40
    },
    {
      "epoch": 0.41379310344827586,
      "grad_norm": 0.005794069729745388,
      "learning_rate": 0.4896551724137931,
      "loss": 2.3146,
      "step": 60
    },
    {
      "epoch": 0.5517241379310345,
      "grad_norm": 0.007081185467541218,
      "learning_rate": 0.4862068965517241,
      "loss": 1.9668,
      "step": 80
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 0.004596109502017498,
      "learning_rate": 0.4827586206896552,
      "loss": 1.7387,
      "step": 100
    },
    {
      "epoch": 0.8275862068965517,
      "grad_norm": 0.024540038779377937,
      "learning_rate": 0.4793103448275862,
      "loss": 1.6736,
      "step": 120
    },
    {
      "epoch": 0.9655172413793104,
      "grad_norm": 0.005169321782886982,
      "learning_rate": 0.47586206896551725,
      "loss": 1.6136,
      "step": 140
    },
    {
      "epoch": 1.103448275862069,
      "grad_norm": 0.006497339811176062,
      "learning_rate": 0.4724137931034483,
      "loss": 1.6371,
      "step": 160
    },
    {
      "epoch": 1.2413793103448276,
      "grad_norm": 0.00513405492529273,
      "learning_rate": 0.4689655172413793,
      "loss": 1.6556,
      "step": 180
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 0.004359169397503138,
      "learning_rate": 0.46551724137931033,
      "loss": 1.5748,
      "step": 200
    },
    {
      "epoch": 1.5172413793103448,
      "grad_norm": 0.005438263528048992,
      "learning_rate": 0.46206896551724136,
      "loss": 1.665,
      "step": 220
    },
    {
      "epoch": 1.6551724137931034,
      "grad_norm": 0.005137151572853327,
      "learning_rate": 0.4586206896551724,
      "loss": 1.6099,
      "step": 240
    },
    {
      "epoch": 1.793103448275862,
      "grad_norm": 0.002719143172726035,
      "learning_rate": 0.45517241379310347,
      "loss": 1.6586,
      "step": 260
    },
    {
      "epoch": 1.9310344827586206,
      "grad_norm": 0.004894926678389311,
      "learning_rate": 0.4517241379310345,
      "loss": 1.6698,
      "step": 280
    },
    {
      "epoch": 2.0689655172413794,
      "grad_norm": 0.009756403043866158,
      "learning_rate": 0.4482758620689655,
      "loss": 1.6887,
      "step": 300
    },
    {
      "epoch": 2.206896551724138,
      "grad_norm": 0.007531631737947464,
      "learning_rate": 0.44482758620689655,
      "loss": 1.6062,
      "step": 320
    },
    {
      "epoch": 2.344827586206897,
      "grad_norm": 0.007430747617036104,
      "learning_rate": 0.4413793103448276,
      "loss": 1.7011,
      "step": 340
    },
    {
      "epoch": 2.4827586206896552,
      "grad_norm": 0.009350535459816456,
      "learning_rate": 0.4379310344827586,
      "loss": 1.6338,
      "step": 360
    },
    {
      "epoch": 2.6206896551724137,
      "grad_norm": 0.004193771164864302,
      "learning_rate": 0.43448275862068964,
      "loss": 1.6706,
      "step": 380
    },
    {
      "epoch": 2.7586206896551726,
      "grad_norm": 0.005513598211109638,
      "learning_rate": 0.43103448275862066,
      "loss": 1.637,
      "step": 400
    },
    {
      "epoch": 2.896551724137931,
      "grad_norm": 0.005511182360351086,
      "learning_rate": 0.42758620689655175,
      "loss": 1.654,
      "step": 420
    },
    {
      "epoch": 3.0344827586206895,
      "grad_norm": 0.003740502754226327,
      "learning_rate": 0.4241379310344828,
      "loss": 1.7207,
      "step": 440
    },
    {
      "epoch": 3.1724137931034484,
      "grad_norm": 0.0038279425352811813,
      "learning_rate": 0.4206896551724138,
      "loss": 1.6529,
      "step": 460
    },
    {
      "epoch": 3.310344827586207,
      "grad_norm": 0.006348178256303072,
      "learning_rate": 0.41724137931034483,
      "loss": 1.6617,
      "step": 480
    },
    {
      "epoch": 3.4482758620689653,
      "grad_norm": 0.008164280094206333,
      "learning_rate": 0.41379310344827586,
      "loss": 1.6182,
      "step": 500
    },
    {
      "epoch": 3.586206896551724,
      "grad_norm": 0.004924620036035776,
      "learning_rate": 0.4103448275862069,
      "loss": 1.6411,
      "step": 520
    },
    {
      "epoch": 3.7241379310344827,
      "grad_norm": 0.0035281360615044832,
      "learning_rate": 0.4068965517241379,
      "loss": 1.6135,
      "step": 540
    },
    {
      "epoch": 3.862068965517241,
      "grad_norm": 0.004006173927336931,
      "learning_rate": 0.40344827586206894,
      "loss": 1.7426,
      "step": 560
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.0035784486681222916,
      "learning_rate": 0.4,
      "loss": 1.6809,
      "step": 580
    },
    {
      "epoch": 4.137931034482759,
      "grad_norm": 0.021265313029289246,
      "learning_rate": 0.39655172413793105,
      "loss": 1.6874,
      "step": 600
    },
    {
      "epoch": 4.275862068965517,
      "grad_norm": 0.006073353346437216,
      "learning_rate": 0.3931034482758621,
      "loss": 1.692,
      "step": 620
    },
    {
      "epoch": 4.413793103448276,
      "grad_norm": 0.0046775382943451405,
      "learning_rate": 0.3896551724137931,
      "loss": 1.6048,
      "step": 640
    },
    {
      "epoch": 4.551724137931035,
      "grad_norm": 0.0029314295388758183,
      "learning_rate": 0.38620689655172413,
      "loss": 1.6145,
      "step": 660
    },
    {
      "epoch": 4.689655172413794,
      "grad_norm": 0.002731095068156719,
      "learning_rate": 0.38275862068965516,
      "loss": 1.5997,
      "step": 680
    },
    {
      "epoch": 4.827586206896552,
      "grad_norm": 0.007168998476117849,
      "learning_rate": 0.3793103448275862,
      "loss": 1.5647,
      "step": 700
    },
    {
      "epoch": 4.9655172413793105,
      "grad_norm": 0.003911875654011965,
      "learning_rate": 0.3758620689655172,
      "loss": 1.6059,
      "step": 720
    },
    {
      "epoch": 5.103448275862069,
      "grad_norm": 0.0021219810005277395,
      "learning_rate": 0.3724137931034483,
      "loss": 1.6252,
      "step": 740
    },
    {
      "epoch": 5.241379310344827,
      "grad_norm": 0.004675926174968481,
      "learning_rate": 0.3689655172413793,
      "loss": 1.5847,
      "step": 760
    },
    {
      "epoch": 5.379310344827586,
      "grad_norm": 0.0030781119130551815,
      "learning_rate": 0.36551724137931035,
      "loss": 1.6239,
      "step": 780
    },
    {
      "epoch": 5.517241379310345,
      "grad_norm": 0.014627201482653618,
      "learning_rate": 0.3620689655172414,
      "loss": 1.6263,
      "step": 800
    },
    {
      "epoch": 5.655172413793103,
      "grad_norm": 0.0018710256554186344,
      "learning_rate": 0.3586206896551724,
      "loss": 1.5907,
      "step": 820
    },
    {
      "epoch": 5.793103448275862,
      "grad_norm": 0.008687611669301987,
      "learning_rate": 0.35517241379310344,
      "loss": 1.5643,
      "step": 840
    },
    {
      "epoch": 5.931034482758621,
      "grad_norm": 0.01578678749501705,
      "learning_rate": 0.35172413793103446,
      "loss": 1.5206,
      "step": 860
    },
    {
      "epoch": 6.068965517241379,
      "grad_norm": 0.014950994402170181,
      "learning_rate": 0.3482758620689655,
      "loss": 1.5794,
      "step": 880
    },
    {
      "epoch": 6.206896551724138,
      "grad_norm": 0.0065665640868246555,
      "learning_rate": 0.3448275862068966,
      "loss": 1.5506,
      "step": 900
    },
    {
      "epoch": 6.344827586206897,
      "grad_norm": 0.005612093023955822,
      "learning_rate": 0.3413793103448276,
      "loss": 1.5795,
      "step": 920
    },
    {
      "epoch": 6.482758620689655,
      "grad_norm": 0.00703503517434001,
      "learning_rate": 0.33793103448275863,
      "loss": 1.5838,
      "step": 940
    },
    {
      "epoch": 6.620689655172414,
      "grad_norm": 0.012617165222764015,
      "learning_rate": 0.33448275862068966,
      "loss": 1.6365,
      "step": 960
    },
    {
      "epoch": 6.758620689655173,
      "grad_norm": 0.034573595970869064,
      "learning_rate": 0.3310344827586207,
      "loss": 1.6173,
      "step": 980
    },
    {
      "epoch": 6.896551724137931,
      "grad_norm": 0.022389907389879227,
      "learning_rate": 0.3275862068965517,
      "loss": 1.5821,
      "step": 1000
    },
    {
      "epoch": 7.0344827586206895,
      "grad_norm": 0.014414334669709206,
      "learning_rate": 0.32413793103448274,
      "loss": 1.5843,
      "step": 1020
    },
    {
      "epoch": 7.172413793103448,
      "grad_norm": 0.010868263430893421,
      "learning_rate": 0.32068965517241377,
      "loss": 1.5917,
      "step": 1040
    },
    {
      "epoch": 7.310344827586207,
      "grad_norm": 0.00919910054653883,
      "learning_rate": 0.31724137931034485,
      "loss": 1.5939,
      "step": 1060
    },
    {
      "epoch": 7.448275862068965,
      "grad_norm": 0.010387918911874294,
      "learning_rate": 0.3137931034482759,
      "loss": 1.6037,
      "step": 1080
    },
    {
      "epoch": 7.586206896551724,
      "grad_norm": 0.005795859266072512,
      "learning_rate": 0.3103448275862069,
      "loss": 1.611,
      "step": 1100
    },
    {
      "epoch": 7.724137931034483,
      "grad_norm": 0.01636197231709957,
      "learning_rate": 0.30689655172413793,
      "loss": 1.5457,
      "step": 1120
    },
    {
      "epoch": 7.862068965517241,
      "grad_norm": 0.025658033788204193,
      "learning_rate": 0.30344827586206896,
      "loss": 1.6961,
      "step": 1140
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.008104110136628151,
      "learning_rate": 0.3,
      "loss": 1.5867,
      "step": 1160
    },
    {
      "epoch": 8.137931034482758,
      "grad_norm": 0.012119215913116932,
      "learning_rate": 0.296551724137931,
      "loss": 1.6717,
      "step": 1180
    },
    {
      "epoch": 8.275862068965518,
      "grad_norm": 0.04409046843647957,
      "learning_rate": 0.29310344827586204,
      "loss": 1.5878,
      "step": 1200
    },
    {
      "epoch": 8.413793103448276,
      "grad_norm": 0.005737460218369961,
      "learning_rate": 0.2896551724137931,
      "loss": 1.5661,
      "step": 1220
    },
    {
      "epoch": 8.551724137931034,
      "grad_norm": 0.0050183492712676525,
      "learning_rate": 0.28620689655172415,
      "loss": 1.5953,
      "step": 1240
    },
    {
      "epoch": 8.689655172413794,
      "grad_norm": 0.0018614362925291061,
      "learning_rate": 0.2827586206896552,
      "loss": 1.5909,
      "step": 1260
    },
    {
      "epoch": 8.827586206896552,
      "grad_norm": 0.005694311112165451,
      "learning_rate": 0.2793103448275862,
      "loss": 1.5186,
      "step": 1280
    },
    {
      "epoch": 8.96551724137931,
      "grad_norm": 0.0072829267010092735,
      "learning_rate": 0.27586206896551724,
      "loss": 1.5531,
      "step": 1300
    },
    {
      "epoch": 9.10344827586207,
      "grad_norm": 0.0035627626348286867,
      "learning_rate": 0.27241379310344827,
      "loss": 1.5527,
      "step": 1320
    },
    {
      "epoch": 9.241379310344827,
      "grad_norm": 0.005137620028108358,
      "learning_rate": 0.2689655172413793,
      "loss": 1.5554,
      "step": 1340
    },
    {
      "epoch": 9.379310344827585,
      "grad_norm": 0.0030467426404356956,
      "learning_rate": 0.2655172413793103,
      "loss": 1.5731,
      "step": 1360
    },
    {
      "epoch": 9.517241379310345,
      "grad_norm": 0.003630711231380701,
      "learning_rate": 0.2620689655172414,
      "loss": 1.5602,
      "step": 1380
    },
    {
      "epoch": 9.655172413793103,
      "grad_norm": 0.0041709356009960175,
      "learning_rate": 0.25862068965517243,
      "loss": 1.5895,
      "step": 1400
    },
    {
      "epoch": 9.793103448275861,
      "grad_norm": 0.005584655329585075,
      "learning_rate": 0.25517241379310346,
      "loss": 1.5313,
      "step": 1420
    },
    {
      "epoch": 9.931034482758621,
      "grad_norm": 0.0044827633537352085,
      "learning_rate": 0.2517241379310345,
      "loss": 1.5581,
      "step": 1440
    },
    {
      "epoch": 10.068965517241379,
      "grad_norm": 0.0037936721928417683,
      "learning_rate": 0.2482758620689655,
      "loss": 1.5558,
      "step": 1460
    },
    {
      "epoch": 10.206896551724139,
      "grad_norm": 0.0021422449499368668,
      "learning_rate": 0.24482758620689654,
      "loss": 1.5657,
      "step": 1480
    },
    {
      "epoch": 10.344827586206897,
      "grad_norm": 0.0073448424227535725,
      "learning_rate": 0.2413793103448276,
      "loss": 1.5499,
      "step": 1500
    },
    {
      "epoch": 10.482758620689655,
      "grad_norm": 0.002134874230250716,
      "learning_rate": 0.23793103448275862,
      "loss": 1.5158,
      "step": 1520
    },
    {
      "epoch": 10.620689655172415,
      "grad_norm": 0.002587741008028388,
      "learning_rate": 0.23448275862068965,
      "loss": 1.5938,
      "step": 1540
    },
    {
      "epoch": 10.758620689655173,
      "grad_norm": 0.0023279860615730286,
      "learning_rate": 0.23103448275862068,
      "loss": 1.6106,
      "step": 1560
    },
    {
      "epoch": 10.89655172413793,
      "grad_norm": 0.002888083690777421,
      "learning_rate": 0.22758620689655173,
      "loss": 1.5937,
      "step": 1580
    },
    {
      "epoch": 11.03448275862069,
      "grad_norm": 0.002341517014428973,
      "learning_rate": 0.22413793103448276,
      "loss": 1.5473,
      "step": 1600
    },
    {
      "epoch": 11.172413793103448,
      "grad_norm": 0.0029775602743029594,
      "learning_rate": 0.2206896551724138,
      "loss": 1.5973,
      "step": 1620
    },
    {
      "epoch": 11.310344827586206,
      "grad_norm": 0.0025544443633407354,
      "learning_rate": 0.21724137931034482,
      "loss": 1.5319,
      "step": 1640
    },
    {
      "epoch": 11.448275862068966,
      "grad_norm": 0.0027832691557705402,
      "learning_rate": 0.21379310344827587,
      "loss": 1.5672,
      "step": 1660
    },
    {
      "epoch": 11.586206896551724,
      "grad_norm": 0.0017047225264832377,
      "learning_rate": 0.2103448275862069,
      "loss": 1.5657,
      "step": 1680
    },
    {
      "epoch": 11.724137931034482,
      "grad_norm": 0.0027595197316259146,
      "learning_rate": 0.20689655172413793,
      "loss": 1.5829,
      "step": 1700
    },
    {
      "epoch": 11.862068965517242,
      "grad_norm": 0.003137665567919612,
      "learning_rate": 0.20344827586206896,
      "loss": 1.5913,
      "step": 1720
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.005637073889374733,
      "learning_rate": 0.2,
      "loss": 1.5724,
      "step": 1740
    },
    {
      "epoch": 12.137931034482758,
      "grad_norm": 0.058529581874608994,
      "learning_rate": 0.19655172413793104,
      "loss": 1.5281,
      "step": 1760
    },
    {
      "epoch": 12.275862068965518,
      "grad_norm": 0.0031933861318975687,
      "learning_rate": 0.19310344827586207,
      "loss": 1.5751,
      "step": 1780
    },
    {
      "epoch": 12.413793103448276,
      "grad_norm": 0.013133494183421135,
      "learning_rate": 0.1896551724137931,
      "loss": 1.5591,
      "step": 1800
    },
    {
      "epoch": 12.551724137931034,
      "grad_norm": 0.0025350800715386868,
      "learning_rate": 0.18620689655172415,
      "loss": 1.5944,
      "step": 1820
    },
    {
      "epoch": 12.689655172413794,
      "grad_norm": 0.0015532182296738029,
      "learning_rate": 0.18275862068965518,
      "loss": 1.5475,
      "step": 1840
    },
    {
      "epoch": 12.827586206896552,
      "grad_norm": 0.00160645239520818,
      "learning_rate": 0.1793103448275862,
      "loss": 1.6179,
      "step": 1860
    },
    {
      "epoch": 12.96551724137931,
      "grad_norm": 0.003616021014750004,
      "learning_rate": 0.17586206896551723,
      "loss": 1.5496,
      "step": 1880
    },
    {
      "epoch": 13.10344827586207,
      "grad_norm": 0.004453522618860006,
      "learning_rate": 0.1724137931034483,
      "loss": 1.6111,
      "step": 1900
    },
    {
      "epoch": 13.241379310344827,
      "grad_norm": 0.003808347973972559,
      "learning_rate": 0.16896551724137931,
      "loss": 1.5141,
      "step": 1920
    },
    {
      "epoch": 13.379310344827585,
      "grad_norm": 0.00500386580824852,
      "learning_rate": 0.16551724137931034,
      "loss": 1.6053,
      "step": 1940
    },
    {
      "epoch": 13.517241379310345,
      "grad_norm": 0.002750847488641739,
      "learning_rate": 0.16206896551724137,
      "loss": 1.5392,
      "step": 1960
    },
    {
      "epoch": 13.655172413793103,
      "grad_norm": 0.0028064600192010403,
      "learning_rate": 0.15862068965517243,
      "loss": 1.5377,
      "step": 1980
    },
    {
      "epoch": 13.793103448275861,
      "grad_norm": 0.0012535094283521175,
      "learning_rate": 0.15517241379310345,
      "loss": 1.5153,
      "step": 2000
    },
    {
      "epoch": 13.931034482758621,
      "grad_norm": 0.0037188371643424034,
      "learning_rate": 0.15172413793103448,
      "loss": 1.6128,
      "step": 2020
    },
    {
      "epoch": 14.068965517241379,
      "grad_norm": 0.003236849792301655,
      "learning_rate": 0.1482758620689655,
      "loss": 1.5732,
      "step": 2040
    },
    {
      "epoch": 14.206896551724139,
      "grad_norm": 0.004577662330120802,
      "learning_rate": 0.14482758620689656,
      "loss": 1.5395,
      "step": 2060
    },
    {
      "epoch": 14.344827586206897,
      "grad_norm": 0.002505010925233364,
      "learning_rate": 0.1413793103448276,
      "loss": 1.6018,
      "step": 2080
    },
    {
      "epoch": 14.482758620689655,
      "grad_norm": 0.0025772370863705873,
      "learning_rate": 0.13793103448275862,
      "loss": 1.5208,
      "step": 2100
    },
    {
      "epoch": 14.620689655172415,
      "grad_norm": 0.0031899926252663136,
      "learning_rate": 0.13448275862068965,
      "loss": 1.6194,
      "step": 2120
    },
    {
      "epoch": 14.758620689655173,
      "grad_norm": 0.0033775053452700377,
      "learning_rate": 0.1310344827586207,
      "loss": 1.5574,
      "step": 2140
    },
    {
      "epoch": 14.89655172413793,
      "grad_norm": 0.003082068869844079,
      "learning_rate": 0.12758620689655173,
      "loss": 1.5981,
      "step": 2160
    },
    {
      "epoch": 15.03448275862069,
      "grad_norm": 0.0013595321215689182,
      "learning_rate": 0.12413793103448276,
      "loss": 1.5534,
      "step": 2180
    },
    {
      "epoch": 15.172413793103448,
      "grad_norm": 0.002460446208715439,
      "learning_rate": 0.1206896551724138,
      "loss": 1.5696,
      "step": 2200
    },
    {
      "epoch": 15.310344827586206,
      "grad_norm": 0.005572505760937929,
      "learning_rate": 0.11724137931034483,
      "loss": 1.558,
      "step": 2220
    },
    {
      "epoch": 15.448275862068966,
      "grad_norm": 0.0026755379512906075,
      "learning_rate": 0.11379310344827587,
      "loss": 1.5821,
      "step": 2240
    },
    {
      "epoch": 15.586206896551724,
      "grad_norm": 0.005194938741624355,
      "learning_rate": 0.1103448275862069,
      "loss": 1.5515,
      "step": 2260
    },
    {
      "epoch": 15.724137931034482,
      "grad_norm": 0.0018104353221133351,
      "learning_rate": 0.10689655172413794,
      "loss": 1.5588,
      "step": 2280
    },
    {
      "epoch": 15.862068965517242,
      "grad_norm": 0.0016059948829934,
      "learning_rate": 0.10344827586206896,
      "loss": 1.5647,
      "step": 2300
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.0015875252429395914,
      "learning_rate": 0.1,
      "loss": 1.6067,
      "step": 2320
    },
    {
      "epoch": 16.137931034482758,
      "grad_norm": 0.004086721688508987,
      "learning_rate": 0.09655172413793103,
      "loss": 1.5203,
      "step": 2340
    },
    {
      "epoch": 16.275862068965516,
      "grad_norm": 0.0031904014758765697,
      "learning_rate": 0.09310344827586207,
      "loss": 1.5672,
      "step": 2360
    },
    {
      "epoch": 16.413793103448278,
      "grad_norm": 0.0026773980353027582,
      "learning_rate": 0.0896551724137931,
      "loss": 1.5228,
      "step": 2380
    },
    {
      "epoch": 16.551724137931036,
      "grad_norm": 0.00510285934433341,
      "learning_rate": 0.08620689655172414,
      "loss": 1.5356,
      "step": 2400
    },
    {
      "epoch": 16.689655172413794,
      "grad_norm": 0.0033587857615202665,
      "learning_rate": 0.08275862068965517,
      "loss": 1.6239,
      "step": 2420
    },
    {
      "epoch": 16.82758620689655,
      "grad_norm": 0.002603275002911687,
      "learning_rate": 0.07931034482758621,
      "loss": 1.5695,
      "step": 2440
    },
    {
      "epoch": 16.96551724137931,
      "grad_norm": 0.0013386454666033387,
      "learning_rate": 0.07586206896551724,
      "loss": 1.5816,
      "step": 2460
    },
    {
      "epoch": 17.103448275862068,
      "grad_norm": 0.00211818004027009,
      "learning_rate": 0.07241379310344828,
      "loss": 1.5152,
      "step": 2480
    },
    {
      "epoch": 17.24137931034483,
      "grad_norm": 0.0030078371055424213,
      "learning_rate": 0.06896551724137931,
      "loss": 1.5871,
      "step": 2500
    },
    {
      "epoch": 17.379310344827587,
      "grad_norm": 0.0017037100624293089,
      "learning_rate": 0.06551724137931035,
      "loss": 1.6082,
      "step": 2520
    },
    {
      "epoch": 17.517241379310345,
      "grad_norm": 0.0011187749914824963,
      "learning_rate": 0.06206896551724138,
      "loss": 1.5463,
      "step": 2540
    },
    {
      "epoch": 17.655172413793103,
      "grad_norm": 0.004964692983776331,
      "learning_rate": 0.05862068965517241,
      "loss": 1.556,
      "step": 2560
    },
    {
      "epoch": 17.79310344827586,
      "grad_norm": 0.0030810674652457237,
      "learning_rate": 0.05517241379310345,
      "loss": 1.5571,
      "step": 2580
    },
    {
      "epoch": 17.93103448275862,
      "grad_norm": 0.004844089969992638,
      "learning_rate": 0.05172413793103448,
      "loss": 1.5544,
      "step": 2600
    },
    {
      "epoch": 18.06896551724138,
      "grad_norm": 0.002986170118674636,
      "learning_rate": 0.04827586206896552,
      "loss": 1.5691,
      "step": 2620
    },
    {
      "epoch": 18.20689655172414,
      "grad_norm": 0.0035769289825111628,
      "learning_rate": 0.04482758620689655,
      "loss": 1.5711,
      "step": 2640
    },
    {
      "epoch": 18.344827586206897,
      "grad_norm": 0.006094363518059254,
      "learning_rate": 0.041379310344827586,
      "loss": 1.5401,
      "step": 2660
    },
    {
      "epoch": 18.482758620689655,
      "grad_norm": 0.002336889738216996,
      "learning_rate": 0.03793103448275862,
      "loss": 1.5212,
      "step": 2680
    },
    {
      "epoch": 18.620689655172413,
      "grad_norm": 0.0020632250234484673,
      "learning_rate": 0.034482758620689655,
      "loss": 1.5665,
      "step": 2700
    },
    {
      "epoch": 18.75862068965517,
      "grad_norm": 0.002494793152436614,
      "learning_rate": 0.03103448275862069,
      "loss": 1.6233,
      "step": 2720
    },
    {
      "epoch": 18.896551724137932,
      "grad_norm": 0.0038095300551503897,
      "learning_rate": 0.027586206896551724,
      "loss": 1.5836,
      "step": 2740
    },
    {
      "epoch": 19.03448275862069,
      "grad_norm": 0.006485952530056238,
      "learning_rate": 0.02413793103448276,
      "loss": 1.5795,
      "step": 2760
    },
    {
      "epoch": 19.17241379310345,
      "grad_norm": 0.005090749356895685,
      "learning_rate": 0.020689655172413793,
      "loss": 1.5688,
      "step": 2780
    },
    {
      "epoch": 19.310344827586206,
      "grad_norm": 0.003340683411806822,
      "learning_rate": 0.017241379310344827,
      "loss": 1.5706,
      "step": 2800
    },
    {
      "epoch": 19.448275862068964,
      "grad_norm": 0.0016890858532860875,
      "learning_rate": 0.013793103448275862,
      "loss": 1.6014,
      "step": 2820
    },
    {
      "epoch": 19.586206896551722,
      "grad_norm": 0.026087893173098564,
      "learning_rate": 0.010344827586206896,
      "loss": 1.5443,
      "step": 2840
    },
    {
      "epoch": 19.724137931034484,
      "grad_norm": 0.0021328774746507406,
      "learning_rate": 0.006896551724137931,
      "loss": 1.5496,
      "step": 2860
    },
    {
      "epoch": 19.862068965517242,
      "grad_norm": 0.004067645408213139,
      "learning_rate": 0.0034482758620689655,
      "loss": 1.557,
      "step": 2880
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.004845758900046349,
      "learning_rate": 0.0,
      "loss": 1.5699,
      "step": 2900
    }
  ],
  "logging_steps": 20,
  "max_steps": 2900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.62396412045337e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
